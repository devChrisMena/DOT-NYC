{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import wget\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_get = wget.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tar = tarfile.open('aclImdb_v1.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 words in vocabulary\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary: All words used, starting by the most frequent\n",
    "with open('aclImdb/imdb.vocab', encoding=\"UTF-8\") as f:\n",
    "    vocab = [word.rstrip() for word in f]\n",
    "    # Keep only most frequent 5000 words rather than all 90000\n",
    "    # Just saving memory - the long tail occurs too few times\n",
    "    # for the model to learn anything anyway\n",
    "    vocab = vocab[:5000]\n",
    "    print('%d words in vocabulary' % (len(vocab),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_tokens(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"\\\\s\", \" \", text)\n",
    "    text = re.sub(\"[^a-zA-Z' ]\", \"\", text)\n",
    "    tokens = text.split(' ')\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_dataset(dirname):\n",
    "    X, y = [], []\n",
    "    # Review files: neg/0_3.txt neg/10000_4.txt neg/10001_4.txt ...\n",
    "    for y_val, y_label in enumerate(['neg', 'pos']):\n",
    "        y_dir = os.path.join(dirname, y_label)\n",
    "        for fname in os.listdir(y_dir):\n",
    "            fpath = os.path.join(y_dir, fname)\n",
    "            # print('\\r' + fpath + '   ', end='')\n",
    "            with open(fpath, encoding=\"UTF-8\") as f:\n",
    "                tokens = text_tokens(f.read())\n",
    "            X.append(tokens)\n",
    "            y.append(y_val)  # 0 for 'neg', 1 for 'pos'\n",
    "    print()\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_dataset('aclImdb/train/')\n",
    "\n",
    "# We are cheating here - this is a test set, not a validation set.\n",
    "# This is just to make results quickly comparable to outside results\n",
    "# during the tutorial, but you should normally never use the test set\n",
    "# during training, of course!\n",
    "X_val, y_val = load_dataset('aclImdb/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_onehot_vector(tokens):\n",
    "    vector = [0] * len(vocab)\n",
    "    for t in tokens:\n",
    "        try:\n",
    "            vector[vocab.index(t)] = 1\n",
    "        except:\n",
    "            pass  # ignore missing words\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [01:14<00:00, 333.94it/s]\n",
      "100%|██████████| 25000/25000 [01:15<00:00, 331.91it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "X_bow_train = [bow_onehot_vector(x) for x in tqdm(X_train)]\n",
    "X_bow_val = [bow_onehot_vector(x) for x in tqdm(X_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_train_history(history):\n",
    "    best_epoch = np.argmax(history.history['val_acc'])\n",
    "    print('Accuracy (epoch %d): %.4f train, %.4f val' % \\\n",
    "          (best_epoch + 1, history.history['acc'][best_epoch], history.history['val_acc'][best_epoch]))\n",
    "# (Note that sentiment.model is the state after the last epoch rather than best epoch!\n",
    "# Use ModelCheckpointer to restore the best epoch.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bow_input (InputLayer)      [(None, 5000)]            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 5001      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,001\n",
      "Trainable params: 5,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Fitting...\n",
      "Epoch 1/10\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.4518 - accuracy: 0.8310 - val_loss: 0.3624 - val_accuracy: 0.8706\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.3142 - accuracy: 0.8877 - val_loss: 0.3167 - val_accuracy: 0.8791\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.2729 - accuracy: 0.8996 - val_loss: 0.3008 - val_accuracy: 0.8810\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.2504 - accuracy: 0.9081 - val_loss: 0.2947 - val_accuracy: 0.8809\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.2348 - accuracy: 0.9150 - val_loss: 0.2916 - val_accuracy: 0.8813\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.2234 - accuracy: 0.9166 - val_loss: 0.2909 - val_accuracy: 0.8817\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.2144 - accuracy: 0.9219 - val_loss: 0.2927 - val_accuracy: 0.8809\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.2070 - accuracy: 0.9242 - val_loss: 0.2951 - val_accuracy: 0.8795\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.2007 - accuracy: 0.9265 - val_loss: 0.2982 - val_accuracy: 0.8779\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.1955 - accuracy: 0.9288 - val_loss: 0.3006 - val_accuracy: 0.8773\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Activation, Dense, Input\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "class BOWSentimentModel(object):\n",
    "    def __init__(self):\n",
    "        bow = Input(shape=(len(vocab),), name='bow_input')\n",
    "        # weights of all inputs\n",
    "        sentiment = Dense(1)(bow)\n",
    "        # normalize to [0, 1] range\n",
    "        sentiment = Activation('sigmoid')(sentiment)\n",
    "\n",
    "        self.model = Model(inputs=[bow], outputs=[sentiment])\n",
    "        self.model.summary()\n",
    "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    def train(self, X, y, X_val, y_val):\n",
    "        print('Fitting...')\n",
    "        return self.model.fit(np.array(X), np.array(y), validation_data=(np.array(X_val), np.array(y_val)), epochs=10, verbose=1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(np.array(X))\n",
    "    \n",
    "sentiment = BOWSentimentModel()\n",
    "history = sentiment.train(X_bow_train, y_train, X_bow_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, it won’t do ANY of those things. It will just make life more miserable for Bronx residents. Maybe fix a highway, raise the speed limit or get rid of the bike lanes. THAT might actually help residents and businesses in The Bronx.\n",
      "[0.36745435]\n",
      "any updates on the 5th Avenue Bus way #BetterBuses ?\n",
      "[0.562199]\n"
     ]
    }
   ],
   "source": [
    "test_text = 'No, it won’t do ANY of those things. It will just make life more miserable for Bronx residents. Maybe fix a highway, raise the speed limit or get rid of the bike lanes. THAT might actually help residents and businesses in The Bronx.'\n",
    "test_tokens = text_tokens(test_text)\n",
    "print(test_text)\n",
    "print(sentiment.predict([bow_onehot_vector(test_tokens)])[0])\n",
    "\n",
    "test_text = 'any updates on the 5th Avenue Bus way #BetterBuses ?'\n",
    "test_tokens = text_tokens(test_text)\n",
    "print(test_text)\n",
    "print(sentiment.predict([bow_onehot_vector(test_tokens)])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes! People like pedestrian plazas, dont let all the suburbanites that dont live here (just drive in for the day) tell you otherwise. Focus on the ppl that live in NYC, not the ones that commute to it.\n",
      "[0.38242462]\n",
      "@ThunderGawdKen did they inform the community about this?\n",
      "[0.52011335]\n",
      "Im gonna take a walk down there to see if theres any signage. This was touched upon at the last meeting for the Transportation Committee for CB2: DOT is moving without any input from CB2, and tends to move in secrecy for the past few years, especially in this neighborhood 🤦🏿‍♂️\n",
      "[0.6647847]\n"
     ]
    }
   ],
   "source": [
    "test_text = 'Yes! People like pedestrian plazas, dont let all the suburbanites that dont live here (just drive in for the day) tell you otherwise. Focus on the ppl that live in NYC, not the ones that commute to it.'\n",
    "test_tokens = text_tokens(test_text)\n",
    "print(test_text)\n",
    "print(sentiment.predict([bow_onehot_vector(test_tokens)])[0])\n",
    "\n",
    "test_text = '@ThunderGawdKen did they inform the community about this?'\n",
    "test_tokens = text_tokens(test_text)\n",
    "print(test_text)\n",
    "print(sentiment.predict([bow_onehot_vector(test_tokens)])[0])\n",
    "\n",
    "test_text = 'Im gonna take a walk down there to see if theres any signage. This was touched upon at the last meeting for the Transportation Committee for CB2: DOT is moving without any input from CB2, and tends to move in secrecy for the past few years, especially in this neighborhood 🤦🏿‍♂️'\n",
    "test_tokens = text_tokens(test_text)\n",
    "print(test_text)\n",
    "print(sentiment.predict([bow_onehot_vector(test_tokens)])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment.model.save('saved_model/model.h5', save_format='h5')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edf259275ad4a72d4dd5b452264ad5fb2b635233dff2a31edc6ebc740e55e21b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
